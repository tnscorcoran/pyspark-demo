{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1674ecf3-60cf-4567-95dc-4e48a0d50fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, string\n",
    "import os\n",
    "import socket\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffec1815-35e4-4f17-b4f1-0c33f971a2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 19:50:02,005 INFO spark.SparkContext: Running Spark version 3.3.1\n",
      "2023-06-08 19:50:02,101 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-06-08 19:50:02,229 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-06-08 19:50:02,229 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\n",
      "2023-06-08 19:50:02,230 INFO resource.ResourceUtils: ==============================================================\n",
      "2023-06-08 19:50:02,230 INFO spark.SparkContext: Submitted application: PythonWordCount\n",
      "2023-06-08 19:50:02,261 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "2023-06-08 19:50:02,279 INFO resource.ResourceProfile: Limiting resource is cpu\n",
      "2023-06-08 19:50:02,279 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\n",
      "2023-06-08 19:50:02,342 INFO spark.SecurityManager: Changing view acls to: 1000810000\n",
      "2023-06-08 19:50:02,342 INFO spark.SecurityManager: Changing modify acls to: 1000810000\n",
      "2023-06-08 19:50:02,343 INFO spark.SecurityManager: Changing view acls groups to: \n",
      "2023-06-08 19:50:02,343 INFO spark.SecurityManager: Changing modify acls groups to: \n",
      "2023-06-08 19:50:02,344 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(1000810000); groups with view permissions: Set(); users  with modify permissions: Set(1000810000); groups with modify permissions: Set()\n",
      "2023-06-08 19:50:02,681 INFO util.Utils: Successfully started service 'sparkDriver' on port 44979.\n",
      "2023-06-08 19:50:02,714 INFO spark.SparkEnv: Registering MapOutputTracker\n",
      "2023-06-08 19:50:02,756 INFO spark.SparkEnv: Registering BlockManagerMaster\n",
      "2023-06-08 19:50:02,780 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "2023-06-08 19:50:02,781 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "2023-06-08 19:50:02,786 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "2023-06-08 19:50:02,810 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-68444fbd-4dcf-4c62-9f2c-a67c8ecbdfcf\n",
      "2023-06-08 19:50:02,831 INFO memory.MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
      "2023-06-08 19:50:02,850 INFO spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "2023-06-08 19:50:02,903 INFO util.log: Logging initialized @2944ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
      "2023-06-08 19:50:03,058 INFO server.Server: jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 11.0.18+10-LTS\n",
      "2023-06-08 19:50:03,082 INFO server.Server: Started @3124ms\n",
      "2023-06-08 19:50:03,123 INFO server.AbstractConnector: Started ServerConnector@61971441{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2023-06-08 19:50:03,123 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "2023-06-08 19:50:03,150 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d22c69f{/,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,351 INFO executor.Executor: Starting executor ID driver on host jupyter-nb-admin-0\n",
      "2023-06-08 19:50:03,361 INFO executor.Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "2023-06-08 19:50:03,386 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40311.\n",
      "2023-06-08 19:50:03,387 INFO netty.NettyBlockTransferService: Server created on jupyter-nb-admin-0:40311\n",
      "2023-06-08 19:50:03,389 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "2023-06-08 19:50:03,397 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, jupyter-nb-admin-0, 40311, None)\n",
      "2023-06-08 19:50:03,401 INFO storage.BlockManagerMasterEndpoint: Registering block manager jupyter-nb-admin-0:40311 with 434.4 MiB RAM, BlockManagerId(driver, jupyter-nb-admin-0, 40311, None)\n",
      "2023-06-08 19:50:03,406 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, jupyter-nb-admin-0, 40311, None)\n",
      "2023-06-08 19:50:03,407 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, jupyter-nb-admin-0, 40311, None)\n",
      "2023-06-08 19:50:03,600 INFO handler.ContextHandler: Stopped o.s.j.s.ServletContextHandler@d22c69f{/,null,STOPPED,@Spark}\n",
      "2023-06-08 19:50:03,602 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2fce33b4{/jobs,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,603 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@619fab92{/jobs/json,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,605 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@407ec8f8{/jobs/job,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,606 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70e33e20{/jobs/job/json,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,607 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@13c6e933{/stages,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,608 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2cf5fa23{/stages/json,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,611 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2894ffb4{/stages/stage,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,612 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1568368a{/stages/stage/json,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,613 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7af6d269{/stages/pool,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,614 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@255dbf90{/stages/pool/json,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,615 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b47756f{/storage,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,616 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@af8c407{/storage/json,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,618 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@97a4475{/storage/rdd,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,619 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d20fdef{/storage/rdd/json,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,620 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a09c46a{/environment,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,621 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@27166e18{/environment/json,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,622 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@69e7af8c{/executors,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,623 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49fc4d0e{/executors/json,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,625 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@38b9f673{/executors/threadDump,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,626 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3ad1e2a5{/executors/threadDump/json,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,636 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@14fec9e3{/static,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,637 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2527c34c{/,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,640 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5e46ef5c{/api,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,641 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48e022c0{/jobs/job/kill,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,642 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49517740{/stages/stage/kill,null,AVAILABLE,@Spark}\n",
      "2023-06-08 19:50:03,648 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@67e6b5d9{/metrics/json,null,AVAILABLE,@Spark}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path:/opt/app-root/src\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 19:50:04,427 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 308.2 KiB, free 434.1 MiB)\n",
      "2023-06-08 19:50:04,519 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 51.7 KiB, free 434.0 MiB)\n",
      "2023-06-08 19:50:04,523 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on jupyter-nb-admin-0:40311 (size: 51.7 KiB, free: 434.3 MiB)\n",
      "2023-06-08 19:50:04,529 INFO spark.SparkContext: Created broadcast 0 from textFile at NativeMethodAccessorImpl.java:0\n",
      "2023-06-08 19:50:04,873 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2023-06-08 19:50:04,892 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2023-06-08 19:50:04,892 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2023-06-08 19:50:06,785 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2023-06-08 19:50:06,970 INFO spark.SparkContext: Starting job: sortBy at /tmp/ipykernel_767/4018626264.py:32\n",
      "2023-06-08 19:50:07,006 INFO scheduler.DAGScheduler: Registering RDD 3 (reduceByKey at /tmp/ipykernel_767/4018626264.py:32) as input to shuffle 0\n",
      "2023-06-08 19:50:07,012 INFO scheduler.DAGScheduler: Got job 0 (sortBy at /tmp/ipykernel_767/4018626264.py:32) with 2 output partitions\n",
      "2023-06-08 19:50:07,013 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (sortBy at /tmp/ipykernel_767/4018626264.py:32)\n",
      "2023-06-08 19:50:07,014 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\n",
      "2023-06-08 19:50:07,018 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 0)\n",
      "2023-06-08 19:50:07,025 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /tmp/ipykernel_767/4018626264.py:32), which has no missing parents\n",
      "2023-06-08 19:50:07,096 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.2 KiB, free 434.0 MiB)\n",
      "2023-06-08 19:50:07,106 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 434.0 MiB)\n",
      "2023-06-08 19:50:07,107 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on jupyter-nb-admin-0:40311 (size: 7.7 KiB, free: 434.3 MiB)\n",
      "2023-06-08 19:50:07,108 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
      "2023-06-08 19:50:07,122 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (PairwiseRDD[3] at reduceByKey at /tmp/ipykernel_767/4018626264.py:32) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-06-08 19:50:07,124 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 2 tasks resource profile 0\n",
      "2023-06-08 19:50:07,194 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (jupyter-nb-admin-0, executor driver, partition 0, PROCESS_LOCAL, 4496 bytes) taskResourceAssignments Map()\n",
      "2023-06-08 19:50:07,198 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (jupyter-nb-admin-0, executor driver, partition 1, PROCESS_LOCAL, 4496 bytes) taskResourceAssignments Map()\n",
      "2023-06-08 19:50:07,217 INFO executor.Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "2023-06-08 19:50:07,217 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "2023-06-08 19:50:07,338 INFO rdd.HadoopRDD: Input split: s3a://temp-tnscorcoran/shakespeare.txt:2723583+2723583\n",
      "2023-06-08 19:50:07,338 INFO rdd.HadoopRDD: Input split: s3a://temp-tnscorcoran/shakespeare.txt:0+2723583\n",
      "2023-06-08 19:50:10,925 INFO python.PythonRunner: Times: total = 3119, boot = 773, init = 413, finish = 1933\n",
      "2023-06-08 19:50:10,925 INFO python.PythonRunner: Times: total = 2909, boot = 815, init = 38, finish = 2056\n",
      "2023-06-08 19:50:10,969 INFO executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 1669 bytes result sent to driver\n",
      "2023-06-08 19:50:10,969 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 1669 bytes result sent to driver\n",
      "2023-06-08 19:50:10,977 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3800 ms on jupyter-nb-admin-0 (executor driver) (1/2)\n",
      "2023-06-08 19:50:10,979 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 3782 ms on jupyter-nb-admin-0 (executor driver) (2/2)\n",
      "2023-06-08 19:50:10,980 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "2023-06-08 19:50:10,986 INFO python.PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 45605\n",
      "2023-06-08 19:50:10,994 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (reduceByKey at /tmp/ipykernel_767/4018626264.py:32) finished in 3.946 s\n",
      "2023-06-08 19:50:10,995 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-06-08 19:50:10,996 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-06-08 19:50:10,996 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 1)\n",
      "2023-06-08 19:50:10,996 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-06-08 19:50:11,000 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (PythonRDD[6] at sortBy at /tmp/ipykernel_767/4018626264.py:32), which has no missing parents\n",
      "2023-06-08 19:50:11,012 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 11.7 KiB, free 434.0 MiB)\n",
      "2023-06-08 19:50:11,020 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.0 MiB)\n",
      "2023-06-08 19:50:11,022 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on jupyter-nb-admin-0:40311 (size: 6.5 KiB, free: 434.3 MiB)\n",
      "2023-06-08 19:50:11,022 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1513\n",
      "2023-06-08 19:50:11,025 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (PythonRDD[6] at sortBy at /tmp/ipykernel_767/4018626264.py:32) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-06-08 19:50:11,025 INFO scheduler.TaskSchedulerImpl: Adding task set 1.0 with 2 tasks resource profile 0\n",
      "2023-06-08 19:50:11,034 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2) (jupyter-nb-admin-0, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
      "2023-06-08 19:50:11,035 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3) (jupyter-nb-admin-0, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
      "2023-06-08 19:50:11,036 INFO executor.Executor: Running task 0.0 in stage 1.0 (TID 2)\n",
      "2023-06-08 19:50:11,036 INFO executor.Executor: Running task 1.0 in stage 1.0 (TID 3)\n",
      "2023-06-08 19:50:11,125 INFO storage.ShuffleBlockFetcherIterator: Getting 2 (265.1 KiB) non-empty blocks including 2 (265.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "2023-06-08 19:50:11,125 INFO storage.ShuffleBlockFetcherIterator: Getting 2 (265.1 KiB) non-empty blocks including 2 (265.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "2023-06-08 19:50:11,129 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 14 ms\n",
      "2023-06-08 19:50:11,129 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 16 ms\n",
      "2023-06-08 19:50:11,183 INFO python.PythonRunner: Times: total = 48, boot = -451, init = 464, finish = 35\n",
      "2023-06-08 19:50:11,185 INFO python.PythonRunner: Times: total = 48, boot = -197, init = 210, finish = 35\n",
      "2023-06-08 19:50:11,186 INFO executor.Executor: Finished task 1.0 in stage 1.0 (TID 3). 1621 bytes result sent to driver\n",
      "2023-06-08 19:50:11,188 INFO executor.Executor: Finished task 0.0 in stage 1.0 (TID 2). 1621 bytes result sent to driver\n",
      "2023-06-08 19:50:11,189 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 155 ms on jupyter-nb-admin-0 (executor driver) (1/2)\n",
      "2023-06-08 19:50:11,190 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 160 ms on jupyter-nb-admin-0 (executor driver) (2/2)\n",
      "2023-06-08 19:50:11,190 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "2023-06-08 19:50:11,193 INFO scheduler.DAGScheduler: ResultStage 1 (sortBy at /tmp/ipykernel_767/4018626264.py:32) finished in 0.182 s\n",
      "2023-06-08 19:50:11,197 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-06-08 19:50:11,197 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "2023-06-08 19:50:11,200 INFO scheduler.DAGScheduler: Job 0 finished: sortBy at /tmp/ipykernel_767/4018626264.py:32, took 4.229521 s\n",
      "2023-06-08 19:50:11,243 INFO spark.SparkContext: Starting job: sortBy at /tmp/ipykernel_767/4018626264.py:32\n",
      "2023-06-08 19:50:11,245 INFO scheduler.DAGScheduler: Got job 1 (sortBy at /tmp/ipykernel_767/4018626264.py:32) with 2 output partitions\n",
      "2023-06-08 19:50:11,245 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (sortBy at /tmp/ipykernel_767/4018626264.py:32)\n",
      "2023-06-08 19:50:11,245 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
      "2023-06-08 19:50:11,246 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-06-08 19:50:11,247 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (PythonRDD[7] at sortBy at /tmp/ipykernel_767/4018626264.py:32), which has no missing parents\n",
      "2023-06-08 19:50:11,251 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 11.0 KiB, free 434.0 MiB)\n",
      "2023-06-08 19:50:11,261 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 434.0 MiB)\n",
      "2023-06-08 19:50:11,261 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on jupyter-nb-admin-0:40311 (size: 6.3 KiB, free: 434.3 MiB)\n",
      "2023-06-08 19:50:11,262 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\n",
      "2023-06-08 19:50:11,264 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 3 (PythonRDD[7] at sortBy at /tmp/ipykernel_767/4018626264.py:32) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-06-08 19:50:11,264 INFO scheduler.TaskSchedulerImpl: Adding task set 3.0 with 2 tasks resource profile 0\n",
      "2023-06-08 19:50:11,266 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 4) (jupyter-nb-admin-0, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
      "2023-06-08 19:50:11,267 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 5) (jupyter-nb-admin-0, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
      "2023-06-08 19:50:11,268 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 4)\n",
      "2023-06-08 19:50:11,268 INFO executor.Executor: Running task 1.0 in stage 3.0 (TID 5)\n",
      "2023-06-08 19:50:11,284 INFO storage.ShuffleBlockFetcherIterator: Getting 2 (265.1 KiB) non-empty blocks including 2 (265.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "2023-06-08 19:50:11,284 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "2023-06-08 19:50:11,286 INFO storage.ShuffleBlockFetcherIterator: Getting 2 (265.1 KiB) non-empty blocks including 2 (265.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "2023-06-08 19:50:11,287 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "2023-06-08 19:50:11,287 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on jupyter-nb-admin-0:40311 in memory (size: 6.5 KiB, free: 434.3 MiB)\n",
      "2023-06-08 19:50:11,347 INFO python.PythonRunner: Times: total = 57, boot = -91, init = 101, finish = 47\n",
      "2023-06-08 19:50:11,349 INFO python.PythonRunner: Times: total = 64, boot = -88, init = 92, finish = 60\n",
      "2023-06-08 19:50:11,350 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 4). 1716 bytes result sent to driver\n",
      "2023-06-08 19:50:11,352 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 4) in 85 ms on jupyter-nb-admin-0 (executor driver) (1/2)\n",
      "2023-06-08 19:50:11,352 INFO executor.Executor: Finished task 1.0 in stage 3.0 (TID 5). 1722 bytes result sent to driver\n",
      "2023-06-08 19:50:11,353 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 5) in 87 ms on jupyter-nb-admin-0 (executor driver) (2/2)\n",
      "2023-06-08 19:50:11,353 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "2023-06-08 19:50:11,355 INFO scheduler.DAGScheduler: ResultStage 3 (sortBy at /tmp/ipykernel_767/4018626264.py:32) finished in 0.105 s\n",
      "2023-06-08 19:50:11,355 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-06-08 19:50:11,355 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "2023-06-08 19:50:11,356 INFO scheduler.DAGScheduler: Job 1 finished: sortBy at /tmp/ipykernel_767/4018626264.py:32, took 0.112161 s\n",
      "2023-06-08 19:50:11,412 INFO spark.SparkContext: Starting job: collect at /tmp/ipykernel_767/4018626264.py:38\n",
      "2023-06-08 19:50:11,415 INFO scheduler.DAGScheduler: Registering RDD 9 (sortBy at /tmp/ipykernel_767/4018626264.py:32) as input to shuffle 1\n",
      "2023-06-08 19:50:11,415 INFO scheduler.DAGScheduler: Got job 2 (collect at /tmp/ipykernel_767/4018626264.py:38) with 2 output partitions\n",
      "2023-06-08 19:50:11,415 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (collect at /tmp/ipykernel_767/4018626264.py:38)\n",
      "2023-06-08 19:50:11,415 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
      "2023-06-08 19:50:11,416 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 5)\n",
      "2023-06-08 19:50:11,417 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (PairwiseRDD[9] at sortBy at /tmp/ipykernel_767/4018626264.py:32), which has no missing parents\n",
      "2023-06-08 19:50:11,422 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 11.8 KiB, free 434.0 MiB)\n",
      "2023-06-08 19:50:11,431 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 434.0 MiB)\n",
      "2023-06-08 19:50:11,432 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on jupyter-nb-admin-0:40311 (size: 7.1 KiB, free: 434.3 MiB)\n",
      "2023-06-08 19:50:11,433 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\n",
      "2023-06-08 19:50:11,433 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on jupyter-nb-admin-0:40311 in memory (size: 6.3 KiB, free: 434.3 MiB)\n",
      "2023-06-08 19:50:11,434 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (PairwiseRDD[9] at sortBy at /tmp/ipykernel_767/4018626264.py:32) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-06-08 19:50:11,434 INFO scheduler.TaskSchedulerImpl: Adding task set 5.0 with 2 tasks resource profile 0\n",
      "2023-06-08 19:50:11,436 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6) (jupyter-nb-admin-0, executor driver, partition 0, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
      "2023-06-08 19:50:11,437 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7) (jupyter-nb-admin-0, executor driver, partition 1, NODE_LOCAL, 4260 bytes) taskResourceAssignments Map()\n",
      "2023-06-08 19:50:11,438 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 6)\n",
      "2023-06-08 19:50:11,438 INFO executor.Executor: Running task 1.0 in stage 5.0 (TID 7)\n",
      "2023-06-08 19:50:11,451 INFO storage.ShuffleBlockFetcherIterator: Getting 2 (265.1 KiB) non-empty blocks including 2 (265.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "2023-06-08 19:50:11,451 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "2023-06-08 19:50:11,454 INFO storage.ShuffleBlockFetcherIterator: Getting 2 (265.1 KiB) non-empty blocks including 2 (265.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "2023-06-08 19:50:11,455 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\n",
      "2023-06-08 19:50:11,470 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on jupyter-nb-admin-0:40311 in memory (size: 7.7 KiB, free: 434.3 MiB)\n",
      "2023-06-08 19:50:11,525 INFO python.PythonRunner: Times: total = 72, boot = -93, init = 96, finish = 69\n",
      "2023-06-08 19:50:11,530 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 6). 1798 bytes result sent to driver\n",
      "2023-06-08 19:50:11,531 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 95 ms on jupyter-nb-admin-0 (executor driver) (1/2)\n",
      "2023-06-08 19:50:11,556 INFO python.PythonRunner: Times: total = 101, boot = -92, init = 97, finish = 96\n",
      "2023-06-08 19:50:11,561 INFO executor.Executor: Finished task 1.0 in stage 5.0 (TID 7). 1798 bytes result sent to driver\n",
      "2023-06-08 19:50:11,562 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 125 ms on jupyter-nb-admin-0 (executor driver) (2/2)\n",
      "2023-06-08 19:50:11,562 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "2023-06-08 19:50:11,564 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (sortBy at /tmp/ipykernel_767/4018626264.py:32) finished in 0.145 s\n",
      "2023-06-08 19:50:11,564 INFO scheduler.DAGScheduler: looking for newly runnable stages\n",
      "2023-06-08 19:50:11,564 INFO scheduler.DAGScheduler: running: Set()\n",
      "2023-06-08 19:50:11,564 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 6)\n",
      "2023-06-08 19:50:11,564 INFO scheduler.DAGScheduler: failed: Set()\n",
      "2023-06-08 19:50:11,565 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (PythonRDD[12] at collect at /tmp/ipykernel_767/4018626264.py:38), which has no missing parents\n",
      "2023-06-08 19:50:11,568 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 9.4 KiB, free 434.0 MiB)\n",
      "2023-06-08 19:50:11,576 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.6 KiB, free 434.0 MiB)\n",
      "2023-06-08 19:50:11,577 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on jupyter-nb-admin-0:40311 (size: 5.6 KiB, free: 434.3 MiB)\n",
      "2023-06-08 19:50:11,578 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\n",
      "2023-06-08 19:50:11,579 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 6 (PythonRDD[12] at collect at /tmp/ipykernel_767/4018626264.py:38) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-06-08 19:50:11,579 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 2 tasks resource profile 0\n",
      "2023-06-08 19:50:11,581 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8) (jupyter-nb-admin-0, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
      "2023-06-08 19:50:11,581 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 9) (jupyter-nb-admin-0, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
      "2023-06-08 19:50:11,582 INFO executor.Executor: Running task 0.0 in stage 6.0 (TID 8)\n",
      "2023-06-08 19:50:11,582 INFO executor.Executor: Running task 1.0 in stage 6.0 (TID 9)\n",
      "2023-06-08 19:50:11,593 INFO storage.ShuffleBlockFetcherIterator: Getting 2 (320.8 KiB) non-empty blocks including 2 (320.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "2023-06-08 19:50:11,593 INFO storage.ShuffleBlockFetcherIterator: Getting 2 (107.3 KiB) non-empty blocks including 2 (107.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "2023-06-08 19:50:11,593 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "2023-06-08 19:50:11,593 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "2023-06-08 19:50:11,625 INFO python.PythonRunner: Times: total = 31, boot = -57, init = 71, finish = 17\n",
      "2023-06-08 19:50:11,629 INFO executor.Executor: Finished task 0.0 in stage 6.0 (TID 8). 120720 bytes result sent to driver\n",
      "2023-06-08 19:50:11,631 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 50 ms on jupyter-nb-admin-0 (executor driver) (1/2)\n",
      "2023-06-08 19:50:11,726 INFO python.PythonRunner: Times: total = 91, boot = -27, init = 42, finish = 76\n",
      "2023-06-08 19:50:11,735 INFO executor.Executor: Finished task 1.0 in stage 6.0 (TID 9). 443058 bytes result sent to driver\n",
      "2023-06-08 19:50:11,735 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on jupyter-nb-admin-0:40311 in memory (size: 7.1 KiB, free: 434.3 MiB)\n",
      "2023-06-08 19:50:11,736 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 9) in 155 ms on jupyter-nb-admin-0 (executor driver) (2/2)\n",
      "2023-06-08 19:50:11,736 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "2023-06-08 19:50:11,738 INFO scheduler.DAGScheduler: ResultStage 6 (collect at /tmp/ipykernel_767/4018626264.py:38) finished in 0.171 s\n",
      "2023-06-08 19:50:11,738 INFO scheduler.DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-06-08 19:50:11,738 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "2023-06-08 19:50:11,739 INFO scheduler.DAGScheduler: Job 2 finished: collect at /tmp/ipykernel_767/4018626264.py:38, took 0.326306 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :  : 679849 \n",
      "1 : the : 27507 \n",
      "2 : and : 26705 \n",
      "3 : i : 20191 \n",
      "4 : to : 19294 \n",
      "5 : of : 18076 \n",
      "6 : a : 14502 \n",
      "7 : you : 12957 \n",
      "8 : my : 12468 \n",
      "9 : that : 10950 \n",
      "10 : in : 10903 \n",
      "11 : is : 9473 \n",
      "12 : not : 8443 \n",
      "13 : for : 8181 \n",
      "14 : with : 7965 \n",
      "15 : it : 7212 \n",
      "16 : be : 6963 \n",
      "17 : me : 6962 \n",
      "18 : your : 6865 \n",
      "19 : his : 6825 \n",
      "20 : this : 6276 \n",
      "21 : but : 6267 \n",
      "22 : he : 6102 \n",
      "23 : as : 5927 \n",
      "24 : have : 5838 \n",
      "25 : thou : 5378 \n",
      "26 : so : 4948 \n",
      "27 : will : 4858 \n",
      "28 : him : 4625 \n",
      "29 : by : 4386 \n",
      "30 : what : 4338 \n",
      "31 : thy : 4031 \n",
      "32 : are : 3802 \n",
      "33 : all : 3792 \n",
      "34 : do : 3637 \n",
      "35 : no : 3608 \n",
      "36 : her : 3585 \n",
      "37 : shall : 3573 \n",
      "38 : if : 3482 \n",
      "39 : we : 3269 \n",
      "40 : or : 3078 \n",
      "41 : our : 3060 \n",
      "42 : on : 2957 \n",
      "43 : thee : 2858 \n",
      "44 : good : 2758 \n",
      "45 : king : 2679 \n",
      "46 : from : 2635 \n",
      "47 : lord : 2626 \n",
      "48 : now : 2546 \n",
      "49 : at : 2491 \n",
      "50 : come : 2397 \n",
      "51 : they : 2379 \n",
      "52 : sir : 2334 \n",
      "53 : enter : 2331 \n",
      "54 : which : 2314 \n",
      "55 : would : 2284 \n",
      "56 : was : 2213 \n",
      "57 : o : 2199 \n",
      "58 : more : 2174 \n",
      "59 : she : 2162 \n",
      "60 : am : 2148 \n",
      "61 : let : 2096 \n",
      "62 : how : 2095 \n",
      "63 : then : 2075 \n",
      "64 : their : 2075 \n",
      "65 : well : 2028 \n",
      "66 : when : 2027 \n",
      "67 : hath : 1936 \n",
      "68 : love : 1917 \n",
      "69 : than : 1880 \n",
      "70 : here : 1869 \n",
      "71 : may : 1832 \n",
      "72 : an : 1831 \n",
      "73 : them : 1798 \n",
      "74 : like : 1766 \n",
      "75 : i'll : 1742 \n",
      "76 : one : 1719 \n",
      "77 : upon : 1712 \n",
      "78 : there : 1651 \n",
      "79 : go : 1641 \n",
      "80 : man : 1627 \n",
      "81 : make : 1626 \n",
      "82 : know : 1619 \n",
      "83 : such : 1609 \n",
      "84 : did : 1600 \n",
      "85 : should : 1567 \n",
      "86 : say : 1565 \n",
      "87 : were : 1562 \n",
      "88 : yet : 1518 \n",
      "89 : must : 1480 \n",
      "90 : us : 1472 \n",
      "91 : why : 1449 \n",
      "92 : had : 1412 \n",
      "93 : 'tis : 1388 \n",
      "94 : see : 1381 \n",
      "95 : out : 1341 \n",
      "96 : some : 1330 \n",
      "97 : give : 1313 \n",
      "98 : these : 1296 \n",
      "99 : time : 1228 \n",
      "100 : where : 1207 \n",
      "101 : take : 1192 \n",
      "102 : can : 1191 \n",
      "103 : most : 1173 \n",
      "104 : who : 1169 \n",
      "105 : th' : 1148 \n",
      "106 : first : 1141 \n",
      "107 : too : 1117 \n",
      "108 : speak : 1096 \n",
      "109 : mine : 1091 \n",
      "110 : tell : 1040 \n",
      "111 : up : 1035 \n",
      "112 : duke : 1031 \n",
      "113 : think : 1014 \n",
      "114 : never : 1010 \n",
      "115 : much : 1001 \n",
      "116 : exeunt : 975 \n",
      "117 : nor : 968 \n",
      "118 : doth : 937 \n",
      "119 : any : 915 \n",
      "120 : art : 896 \n",
      "121 : great : 893 \n",
      "122 : heart : 890 \n",
      "123 : queen : 880 \n",
      "124 : exit : 865 \n",
      "125 : hear : 836 \n",
      "126 : very : 813 \n",
      "127 : made : 813 \n",
      "128 : before : 807 \n",
      "129 : look : 806 \n",
      "130 : sweet : 786 \n",
      "131 : men : 786 \n",
      "132 : day : 783 \n",
      "133 : world : 774 \n",
      "134 : scene : 766 \n",
      "135 : cannot : 760 \n",
      "136 : lady : 752 \n",
      "137 : death : 751 \n",
      "138 : life : 747 \n",
      "139 : own : 744 \n",
      "140 : hand : 739 \n",
      "141 : father : 737 \n",
      "142 : god : 730 \n",
      "143 : fair : 727 \n",
      "144 : pray : 719 \n",
      "145 : ay : 719 \n",
      "146 : prince : 713 \n",
      "147 : master : 705 \n",
      "148 : old : 701 \n",
      "149 : true : 698 \n",
      "150 : two : 692 \n",
      "151 : away : 689 \n",
      "152 : been : 681 \n",
      "153 : night : 672 \n",
      "154 : thus : 663 \n",
      "155 : being : 658 \n",
      "156 : long : 656 \n",
      "157 : other : 652 \n",
      "158 : into : 637 \n",
      "159 : again : 635 \n",
      "160 : leave : 635 \n",
      "161 : therefore : 623 \n",
      "162 : could : 621 \n",
      "163 : though : 618 \n",
      "164 : whose : 616 \n",
      "165 : till : 615 \n",
      "166 : poor : 614 \n",
      "167 : both : 610 \n",
      "168 : son : 608 \n",
      "169 : eyes : 607 \n",
      "170 : noble : 607 \n",
      "171 : blood : 602 \n",
      "172 : ' : 601 \n",
      "173 : against : 593 \n",
      "174 : ever : 592 \n",
      "175 : down : 589 \n",
      "176 : fear : 587 \n",
      "177 : comes : 587 \n",
      "178 : nothing : 584 \n",
      "179 : hast : 583 \n",
      "180 : nay : 579 \n",
      "181 : second : 571 \n",
      "182 : better : 570 \n",
      "183 : even : 567 \n",
      "184 : done : 567 \n",
      "185 : many : 566 \n",
      "186 : heaven : 564 \n",
      "187 : name : 562 \n",
      "188 : henry : 557 \n",
      "189 : those : 545 \n",
      "190 : gloucester : 544 \n",
      "191 : call : 542 \n",
      "192 : stand : 541 \n",
      "193 : use : 534 \n",
      "194 : honour : 529 \n",
      "195 : bear : 528 \n",
      "196 : only : 527 \n",
      "197 : still : 521 \n",
      "198 : find : 517 \n",
      "199 : grace : 515 \n",
      "200 : way : 511 \n",
      "201 : myself : 507 \n",
      "202 : house : 505 \n",
      "203 : brother : 500 \n",
      "204 : john : 497 \n",
      "205 : put : 495 \n",
      "206 : every : 492 \n",
      "207 : might : 485 \n",
      "208 : antony : 476 \n",
      "209 : gentleman : 475 \n",
      "210 : part : 474 \n",
      "211 : richard : 474 \n",
      "212 : keep : 472 \n",
      "213 : live : 468 \n",
      "214 : caesar : 468 \n",
      "215 : little : 468 \n",
      "216 : france : 464 \n",
      "217 : head : 459 \n",
      "218 : york : 456 \n",
      "219 : young : 453 \n",
      "220 : set : 453 \n",
      "221 : brutus : 452 \n",
      "222 : bring : 442 \n",
      "223 : wife : 440 \n",
      "224 : none : 438 \n",
      "225 : since : 438 \n",
      "226 : word : 438 \n",
      "227 : hold : 437 \n",
      "228 : falstaff : 436 \n",
      "229 : copies : 436 \n",
      "230 : distributed : 435 \n",
      "231 : electronic : 434 \n",
      "232 : peace : 430 \n",
      "233 : dear : 430 \n",
      "234 : off : 428 \n",
      "235 : dead : 428 \n",
      "236 : full : 427 \n",
      "237 : thine : 426 \n",
      "238 : best : 426 \n",
      "239 : unto : 424 \n",
      "240 : dost : 423 \n",
      "241 : service : 422 \n",
      "242 : die : 421 \n",
      "243 : madam : 413 \n",
      "244 : whom : 411 \n",
      "245 : friends : 410 \n",
      "246 : after : 407 \n",
      "247 : place : 406 \n",
      "248 : another : 404 \n",
      "249 : show : 403 \n",
      "250 : once : 400 \n",
      "251 : that's : 400 \n",
      "252 : servant : 400 \n",
      "253 : soul : 400 \n",
      "254 : lords : 399 \n",
      "255 : three : 398 \n",
      "256 : himself : 396 \n",
      "257 : there's : 393 \n",
      "258 : else : 388 \n",
      "259 : eye : 388 \n",
      "260 : stay : 387 \n",
      "261 : others : 386 \n",
      "262 : fool : 384 \n",
      "263 : page : 384 \n",
      "264 : about : 384 \n",
      "265 : indeed : 379 \n",
      "266 : mistress : 379 \n",
      "267 : has : 379 \n",
      "268 : please : 377 \n",
      "269 : tongue : 375 \n",
      "270 : warwick : 375 \n",
      "271 : gone : 373 \n",
      "272 : words : 372 \n",
      "273 : i' : 370 \n",
      "274 : friend : 368 \n",
      "275 : gentle : 364 \n",
      "276 : faith : 364 \n",
      "277 : without : 364 \n",
      "278 : within : 360 \n",
      "279 : ere : 360 \n",
      "280 : said : 359 \n",
      "281 : answer : 359 \n",
      "282 : me; : 359 \n",
      "283 : ham : 358 \n",
      "284 : thought : 357 \n",
      "285 : o' : 355 \n",
      "286 : marry : 353 \n",
      "287 : makes : 352 \n",
      "288 : daughter : 350 \n",
      "289 : face : 349 \n",
      "290 : iago : 347 \n",
      "291 : clown : 347 \n",
      "292 : boy : 343 \n",
      "293 : hope : 343 \n",
      "294 : heard : 339 \n",
      "295 : welcome : 339 \n",
      "296 : bid : 337 \n",
      "297 : what's : 337 \n",
      "298 : forth : 335 \n",
      "299 : act : 334 \n",
      "300 : william : 331 \n",
      "301 : end : 326 \n",
      "302 : came : 324 \n",
      "303 : nature : 324 \n",
      "304 : thousand : 320 \n",
      "305 : things : 319 \n",
      "306 : othello : 319 \n",
      "307 : thing : 316 \n",
      "308 : does : 316 \n",
      "309 : meet : 315 \n",
      "310 : mind : 313 \n",
      "311 : rather : 313 \n",
      "312 : we'll : 312 \n",
      "313 : back : 311 \n",
      "314 : right : 310 \n",
      "315 : mother : 310 \n",
      "316 : sword : 309 \n",
      "317 : edward : 308 \n",
      "318 : rest : 303 \n",
      "319 : cause : 303 \n",
      "320 : lie : 302 \n",
      "321 : wilt : 302 \n",
      "322 : get : 300 \n",
      "323 : hence : 300 \n",
      "324 : morrow : 299 \n",
      "325 : messenger : 299 \n",
      "326 : he's : 298 \n",
      "327 : follow : 297 \n",
      "328 : farewell : 296 \n",
      "329 : fortune : 296 \n",
      "330 : shalt : 294 \n",
      "331 : under : 294 \n",
      "332 : ] : 293 \n",
      "333 : timon : 293 \n",
      "334 : home : 291 \n",
      "335 : truth : 290 \n",
      "336 : play : 288 \n",
      "337 : thank : 288 \n",
      "338 : power : 287 \n",
      "339 : rome : 287 \n",
      "340 : hands : 285 \n",
      "341 : sure : 284 \n",
      "342 : woman : 282 \n",
      "343 : false : 280 \n",
      "344 : \" : 279 \n",
      "345 : fall : 277 \n",
      "346 : hour : 277 \n",
      "347 : enough : 277 \n",
      "348 : far : 276 \n",
      "349 : [aside] : 276 \n",
      "350 : fellow : 276 \n",
      "351 : help : 275 \n",
      "352 : macbeth : 275 \n",
      "353 : [to : 274 \n",
      "354 : honest : 273 \n",
      "355 : state : 272 \n",
      "356 : lay : 271 \n",
      "357 : kind : 270 \n",
      "358 : earth : 270 \n",
      "359 : him; : 269 \n",
      "360 : re : 268 \n",
      "361 : ill : 267 \n",
      "362 : pardon : 267 \n",
      "363 : new : 265 \n",
      "364 : ford : 263 \n",
      "365 : ye : 262 \n",
      "366 : sent : 262 \n",
      "367 : shame : 262 \n",
      "368 : you; : 261 \n",
      "369 : you? : 260 \n",
      "370 : mean : 259 \n",
      "371 : turn : 258 \n",
      "372 : through : 258 \n",
      "373 : saw : 258 \n",
      "374 : send : 257 \n",
      "375 : let's : 257 \n",
      "376 : bed : 257 \n",
      "377 : shakespeare : 255 \n",
      "378 : fight : 255 \n",
      "379 : hither : 255 \n",
      "380 : cleopatra : 255 \n",
      "381 : reason : 255 \n",
      "382 : matter : 254 \n",
      "383 : high : 253 \n",
      "384 : seen : 253 \n",
      "385 : break : 253 \n",
      "386 : last : 252 \n",
      "387 : rosalind : 252 \n",
      "388 : means : 251 \n",
      "389 : near : 251 \n",
      "390 : hot : 250 \n",
      "391 : me? : 249 \n",
      "392 : youth : 249 \n",
      "393 : gods : 248 \n",
      "394 : buckingham : 247 \n",
      "395 : prove : 247 \n",
      "396 : swear : 247 \n",
      "397 : yourself : 246 \n",
      "398 : shallow : 245 \n",
      "399 : provided : 245 \n",
      "400 : tears : 242 \n",
      "401 : soldier : 242 \n",
      "402 : left : 240 \n",
      "403 : husband : 239 \n",
      "404 : third : 238 \n",
      "405 : syracuse : 238 \n",
      "406 : light : 238 \n",
      "407 : present : 238 \n",
      "408 : works : 238 \n",
      "409 : used : 237 \n",
      "410 : body : 236 \n",
      "411 : between : 236 \n",
      "412 : beat : 235 \n",
      "413 : each : 235 \n",
      "414 : wit : 235 \n",
      "415 : complete : 235 \n",
      "416 : says : 234 \n",
      "417 : news : 233 \n",
      "418 : half : 232 \n",
      "419 : spirit : 231 \n",
      "420 : charges : 231 \n",
      "421 : suffolk : 231 \n",
      "422 : thoughts : 231 \n",
      "423 : lost : 230 \n",
      "424 : personal : 230 \n",
      "425 : antonio : 230 \n",
      "426 : gave : 229 \n",
      "427 : bloody : 229 \n",
      "428 : lucius : 229 \n",
      "429 : believe : 228 \n",
      "430 : project : 228 \n",
      "431 : war : 228 \n",
      "432 : sleep : 228 \n",
      "433 : father's : 228 \n",
      "434 : fire : 227 \n",
      "435 : england : 227 \n",
      "436 : strange : 226 \n",
      "437 : worthy : 226 \n",
      "438 : while : 226 \n",
      "439 : speed : 225 \n",
      "440 : told : 224 \n",
      "441 : lies : 224 \n",
      "442 : cousin : 224 \n",
      "443 : seek : 223 \n",
      "444 : lear : 223 \n",
      "445 : valentine : 222 \n",
      "446 : o'er : 221 \n",
      "447 : crown : 221 \n",
      "448 : permission : 221 \n",
      "449 : beseech : 221 \n",
      "450 : arms : 221 \n",
      "451 : cassio : 221 \n",
      "452 : library : 220 \n",
      "453 : college : 220 \n",
      "454 : troilus : 220 \n",
      "455 : foul : 220 \n",
      "456 : distribution : 219 \n",
      "457 : same : 219 \n",
      "458 : found : 219 \n",
      "459 : includes : 219 \n",
      "460 : ne'er : 218 \n",
      "461 : machine : 218 \n",
      "462 : <<this : 217 \n",
      "463 : version : 217 \n",
      "464 : copyright : 217 \n",
      "465 : 1990 : 217 \n",
      "466 : 1993 : 217 \n",
      "467 : inc : 217 \n",
      "468 : gutenberg : 217 \n",
      "469 : (2) : 217 \n",
      "470 : prohibited : 217 \n",
      "471 : commercial : 217 \n",
      "472 : desdemona : 217 \n",
      "473 : etext : 217 \n",
      "474 : illinois : 217 \n",
      "475 : benedictine : 217 \n",
      "476 : readable : 217 \n",
      "477 : (1) : 217 \n",
      "478 : commercially : 217 \n",
      "479 : download : 217 \n",
      "480 : membership : 217 \n",
      "481 : >> : 217 \n",
      "482 : justice : 217 \n",
      "483 : yours : 216 \n",
      "484 : sea : 216 \n",
      "485 : fly : 216 \n",
      "486 : court : 216 \n",
      "487 : antipholus : 216 \n",
      "488 : mrs : 216 \n",
      "489 : sir; : 215 \n",
      "490 : cassius : 215 \n",
      "491 : mad : 214 \n",
      "492 : mark : 213 \n",
      "493 : prithee : 213 \n",
      "494 : it; : 213 \n",
      "495 : together : 213 \n",
      "496 : save : 212 \n",
      "497 : itself : 212 \n",
      "498 : proteus : 212 \n",
      "499 : french : 211 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 19:50:11,871 INFO Configuration.deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir\n",
      "2023-06-08 19:50:11,878 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "2023-06-08 19:50:11,880 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "2023-06-08 19:50:11,880 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2023-06-08 19:50:11,941 INFO spark.SparkContext: Starting job: runJob at SparkHadoopWriter.scala:83\n",
      "2023-06-08 19:50:11,944 INFO scheduler.DAGScheduler: Got job 3 (runJob at SparkHadoopWriter.scala:83) with 2 output partitions\n",
      "2023-06-08 19:50:11,944 INFO scheduler.DAGScheduler: Final stage: ResultStage 9 (runJob at SparkHadoopWriter.scala:83)\n",
      "2023-06-08 19:50:11,944 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
      "2023-06-08 19:50:11,944 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2023-06-08 19:50:11,946 INFO scheduler.DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[15] at saveAsTextFile at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "2023-06-08 19:50:11,968 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 156.8 KiB, free 433.9 MiB)\n",
      "2023-06-08 19:50:11,975 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 59.1 KiB, free 433.8 MiB)\n",
      "2023-06-08 19:50:11,976 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on jupyter-nb-admin-0:40311 (size: 59.1 KiB, free: 434.3 MiB)\n",
      "2023-06-08 19:50:11,977 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\n",
      "2023-06-08 19:50:11,977 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on jupyter-nb-admin-0:40311 in memory (size: 5.6 KiB, free: 434.3 MiB)\n",
      "2023-06-08 19:50:11,978 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 9 (MapPartitionsRDD[15] at saveAsTextFile at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))\n",
      "2023-06-08 19:50:11,978 INFO scheduler.TaskSchedulerImpl: Adding task set 9.0 with 2 tasks resource profile 0\n",
      "2023-06-08 19:50:11,980 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 10) (jupyter-nb-admin-0, executor driver, partition 0, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
      "2023-06-08 19:50:11,980 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 9.0 (TID 11) (jupyter-nb-admin-0, executor driver, partition 1, NODE_LOCAL, 4271 bytes) taskResourceAssignments Map()\n",
      "2023-06-08 19:50:11,981 INFO executor.Executor: Running task 0.0 in stage 9.0 (TID 10)\n",
      "2023-06-08 19:50:11,981 INFO executor.Executor: Running task 1.0 in stage 9.0 (TID 11)\n",
      "2023-06-08 19:50:12,052 INFO storage.ShuffleBlockFetcherIterator: Getting 2 (107.3 KiB) non-empty blocks including 2 (107.3 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "2023-06-08 19:50:12,052 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "2023-06-08 19:50:12,055 INFO storage.ShuffleBlockFetcherIterator: Getting 2 (320.8 KiB) non-empty blocks including 2 (320.8 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "2023-06-08 19:50:12,055 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "2023-06-08 19:50:12,059 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "2023-06-08 19:50:12,059 INFO io.HadoopMapRedCommitProtocol: Using output committer class org.apache.hadoop.mapred.FileOutputCommitter\n",
      "2023-06-08 19:50:12,059 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "2023-06-08 19:50:12,060 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2023-06-08 19:50:12,060 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1\n",
      "2023-06-08 19:50:12,060 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2023-06-08 19:50:12,165 INFO python.PythonRunner: Times: total = 76, boot = -416, init = 419, finish = 73\n",
      "2023-06-08 19:50:12,174 INFO output.FileOutputCommitter: Saved output of task 'attempt_202306081950111376973013620545023_0015_m_000000_0' to file:/opt/app-root/src/result/08-06-2023_19:50:11/_temporary/0/task_202306081950111376973013620545023_0015_m_000000\n",
      "2023-06-08 19:50:12,175 INFO mapred.SparkHadoopMapRedUtil: attempt_202306081950111376973013620545023_0015_m_000000_0: Committed. Elapsed time: 1 ms.\n",
      "2023-06-08 19:50:12,178 INFO executor.Executor: Finished task 0.0 in stage 9.0 (TID 10). 1960 bytes result sent to driver\n",
      "2023-06-08 19:50:12,180 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 10) in 201 ms on jupyter-nb-admin-0 (executor driver) (1/2)\n",
      "2023-06-08 19:50:12,238 INFO python.PythonRunner: Times: total = 178, boot = -362, init = 367, finish = 173\n",
      "2023-06-08 19:50:12,241 INFO output.FileOutputCommitter: Saved output of task 'attempt_202306081950111376973013620545023_0015_m_000001_0' to file:/opt/app-root/src/result/08-06-2023_19:50:11/_temporary/0/task_202306081950111376973013620545023_0015_m_000001\n",
      "2023-06-08 19:50:12,241 INFO mapred.SparkHadoopMapRedUtil: attempt_202306081950111376973013620545023_0015_m_000001_0: Committed. Elapsed time: 1 ms.\n",
      "2023-06-08 19:50:12,243 INFO executor.Executor: Finished task 1.0 in stage 9.0 (TID 11). 1960 bytes result sent to driver\n",
      "2023-06-08 19:50:12,244 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 9.0 (TID 11) in 264 ms on jupyter-nb-admin-0 (executor driver) (2/2)\n",
      "2023-06-08 19:50:12,245 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "2023-06-08 19:50:12,246 INFO scheduler.DAGScheduler: ResultStage 9 (runJob at SparkHadoopWriter.scala:83) finished in 0.299 s\n",
      "2023-06-08 19:50:12,246 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2023-06-08 19:50:12,246 INFO scheduler.TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "2023-06-08 19:50:12,247 INFO scheduler.DAGScheduler: Job 3 finished: runJob at SparkHadoopWriter.scala:83, took 0.305496 s\n",
      "2023-06-08 19:50:12,249 INFO io.SparkHadoopWriter: Start to commit write Job job_202306081950111376973013620545023_0015.\n",
      "2023-06-08 19:50:12,272 INFO io.SparkHadoopWriter: Write Job job_202306081950111376973013620545023_0015 committed. Elapsed time: 22 ms.\n",
      "2023-06-08 19:50:12,281 INFO server.AbstractConnector: Stopped Spark@61971441{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}\n",
      "2023-06-08 19:50:12,286 INFO ui.SparkUI: Stopped Spark web UI at http://jupyter-nb-admin-0:4040\n",
      "2023-06-08 19:50:12,304 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "2023-06-08 19:50:12,329 INFO memory.MemoryStore: MemoryStore cleared\n",
      "2023-06-08 19:50:12,330 INFO storage.BlockManager: BlockManager stopped\n",
      "2023-06-08 19:50:12,335 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\n",
      "2023-06-08 19:50:12,338 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "2023-06-08 19:50:12,353 INFO spark.SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonWordCount\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # s3_endpoint_url = os.environ['S3_ENDPOINT_URL']\n",
    "    # s3_access_key_id = os.environ['AWS_ACCESS_KEY_ID']\n",
    "    # s3_secret_access_key = os.environ['AWS_SECRET_ACCESS_KEY']\n",
    "    # s3_bucket = os.environ['BUCKET_NAME']\n",
    "\n",
    "    s3_endpoint_url = \"https://temp-tnscorcoran.s3.ap-southeast-2.amazonaws.com\"\n",
    "    s3_access_key_id = \"AKIASU6WTNELPOKETGSS\"\n",
    "    s3_secret_access_key = \"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\"\n",
    "    s3_bucket = \"temp-tnscorcoran\"\n",
    "\n",
    "    file_path = os.getcwd()\n",
    "    print(\"file path:\" + file_path)\n",
    "\n",
    "    hadoopConf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "    hadoopConf.set(\"fs.s3a.endpoint\", s3_endpoint_url)\n",
    "    hadoopConf.set(\"fs.s3a.access.key\", s3_access_key_id)\n",
    "    hadoopConf.set(\"fs.s3a.secret.key\", s3_secret_access_key)\n",
    "    hadoopConf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "    hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "\n",
    "    text_file = spark.sparkContext.textFile(\"s3a://\" + s3_bucket + \"/shakespeare.txt\") \\\n",
    "                .flatMap(lambda line: line.split(\" \")) \\\n",
    "                .map( lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower())\n",
    "\n",
    "    sorted_counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n",
    "            .map(lambda word: (word, 1)) \\\n",
    "            .reduceByKey(lambda a, b: a + b) \\\n",
    "            .sortBy(lambda wordCounts: wordCounts[1], ascending=False)\n",
    "\n",
    "    i = 0\n",
    "    for word, count in sorted_counts.collect()[0:500]:\n",
    "        print(\"{} : {} : {} \".format(i, word, count))\n",
    "        i += 1\n",
    "\n",
    "    now = datetime.now() # current date and time\n",
    "    date_time = now.strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "\n",
    "    # sorted_counts.saveAsTextFile(\"s3a://\" + s3_bucket + \"/sorted_counts_\" + date_time)\n",
    "    sorted_counts.saveAsTextFile(\"file:///opt/app-root/src/result/\" + date_time)\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5226138e-94d0-4cac-a017-6b541931b2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
